{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Backup Copy] DSP Python Workshop 2022.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1xYXxy9ytJnh",
        "1NqsEYMbtQrI",
        "BdWE3TSdPu_v",
        "0IxIzDKQQebR",
        "QtXwWm2Ju6mr",
        "v3xpkyxixmHr",
        "EXsx9DB00AYW",
        "DHErDZMTMnJ9",
        "ZLGNSrLKymV2",
        "kAED596LT_gz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpacilo/PythonWorkshop/blob/main/Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Better Practices** in Python For Data Science\n",
        "‚ö†Ô∏è Please make a copy of this colab notebook first by clicking **File -> Save a Copy in Drive** on the menu bar <br>"
      ],
      "metadata": {
        "id": "1xYXxy9ytJnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README\n",
        "\n",
        "**Lecturer**\n",
        "- Joshua Paolo Acilo\n",
        "- Model Development Expert\n",
        "- EDO Advanced Analytics\n",
        "\n",
        "**Schedule**\n",
        "- 2:00 - 3:30 PM Lecture\n",
        "- 3:30 - 3:50 PM Quiz\n",
        "- 3:50 - 4:00 PM Q&A\n",
        "\n",
        "**Reminders**\n",
        "- Feel free to ask questions anytime! You can leave a message in the chatbox or unmute yourself and speak. <br>\n",
        "- This is not an Introduction to Python. I expect everyone to at least know the basics in programming. <br>\n",
        "- You learn more by doing. Try to adopt this new concepts in your workflow next time!\n",
        "\n"
      ],
      "metadata": {
        "id": "1NqsEYMbtQrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What to expect from me in this session"
      ],
      "metadata": {
        "id": "BdWE3TSdPu_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This afternoon we're going to talk about üî¨üìà‚öΩ‚ôªÔ∏è"
      ],
      "metadata": {
        "id": "JNfOEmT7P8T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Python"
      ],
      "metadata": {
        "id": "0IxIzDKQQebR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the current python version you have\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "id": "7bSw9y6halM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to mute the warnings for deprecated methods\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "MQFXkYmbdpyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pendulum is a library to manipulate dates\n",
        "%%capture\n",
        "!pip3 install pendulum"
      ],
      "metadata": {
        "id": "9MttWE5QQhui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# geopandas is a library to manipulate spatial data\n",
        "%%capture\n",
        "!pip3 install geopandas"
      ],
      "metadata": {
        "id": "YdGh4gw7YNhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# leafmap is a library to visualize spatial data\n",
        "%%capture\n",
        "!pip3 install leafmap"
      ],
      "metadata": {
        "id": "a9A5RT1Qihlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gives nicer output for your tests\n",
        "%%capture\n",
        "!pip3 -q install pytest pytest-sugar"
      ],
      "metadata": {
        "id": "4K-MqRYxrfai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Clean Code \n",
        "\n",
        "Any fool can write code that a computer can understand. **Good programmers write code that humans can understand.** ü§î"
      ],
      "metadata": {
        "id": "QtXwWm2Ju6mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recap of some things to note when writing Python variables. <br>\n",
        "\n",
        "**DON'T(s)**\n",
        "- Thou shall not start with a number. <br>\n",
        "```4ever = True```\n",
        "- Thou shall not use special characters. <br>\n",
        "```amountIn$ = 100```\n",
        "- Thou shall not use reserved keywords. <br>\n",
        "```id = 10012216```\n",
        "\n",
        "**DO(s)**\n",
        "- PEP8 suggests to use snake_case. <br>\n",
        "```lower_case_with_underscores = True```"
      ],
      "metadata": {
        "id": "zJnr4XvZkAfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use **meaningful and pronounceable variable names.** Let the variable speak for itself. ü§Ø"
      ],
      "metadata": {
        "id": "WyIo8VoTLYvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pendulum\n",
        "\n",
        "def start_pipeline(date):\n",
        "    # do stuff\n",
        "    pass\n",
        "\n",
        "# this is bad, not only it is unpronounceable, it is also vague and non-descriptive\n",
        "ymddt = pendulum.now().strftime(\"%Y-%m-%d\")\n",
        "start_pipeline(ymddt)\n",
        "\n",
        "# this is good, it gives me clue that the current date controls the timing of the pipeline\n",
        "current_date = pendulum.now().strftime(\"%Y-%m-%d\")\n",
        "start_pipeline(current_date)"
      ],
      "metadata": {
        "id": "UgmX9QhIDQzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out current_date variable yourself\n",
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "G0Ab3bBSGEKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, there will be some exceptions, especially in **domain-specific jargons.** üßê"
      ],
      "metadata": {
        "id": "kKBjcYAPDXXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# you'll see this very often in the lake\n",
        "# pxn_dt stands for partition date\n",
        "pxn_dt = pendulum.parse(current_date).subtract(days=1)\n",
        "\n",
        "# this is boilerplate ML, so it's okay too\n",
        "# X is the input matrix (features)\n",
        "# y is the output vector (prediction)\n",
        "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
        "\n",
        "# this one also is widespread in DS\n",
        "# df stands for dataframe \n",
        "df = pd.read_csv(\"sample_data/california_housing_train.csv\")"
      ],
      "metadata": {
        "id": "TsuBSjgLDXQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "94AauiuLHctK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a non-exhaustive list of jargons that aren't that exactly pronounceable and meaningful (for people with no background in data science).\n",
        "- abt => analytical base table\n",
        "- lng => longitude (from lat, lng)\n",
        "- std => standard deviation\n",
        "- txn => transaction\n",
        "- pxn => partition"
      ],
      "metadata": {
        "id": "rz8IXogMJsOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Can you think of a well-accepted variable name in some domains within the Python community **that is not pronounceable?**"
      ],
      "metadata": {
        "id": "Kp6QIxIcHkMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a fact that *we will read more code than we will ever write.* It's important that **the code is readable and searchable.** Yes, we can proceed with the quick and dirty way and get the same result as compared to the slow and cleaner way, but in the long run this will hurt your readers. üòì"
      ],
      "metadata": {
        "id": "dkzK3fLKV-DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_features(window_duration):\n",
        "    # do stuff\n",
        "    pass\n",
        "\n",
        "# i'm betting you'll forget this the next time you look at your code\n",
        "aggregate_features(1440)\n",
        "\n",
        "# we can assign a descriptive constant instead denoted by capital letters \n",
        "MINUTES_IN_A_DAY = 60 * 24\n",
        "aggregate_features(MINUTES_IN_A_DAY)"
      ],
      "metadata": {
        "id": "zfb7U7sHV9oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a constant following the UPPER_CAP_SNAKE_CASE format pertaining to the number of seconds in a day\n",
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "_JGPDXZrK_CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't force the reader of your code to translate what the variable means. **Explicit is better than implicit.** ü§î"
      ],
      "metadata": {
        "id": "kEQdf4fYbt43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is bad, implicit\n",
        "seq = (\"Taguig\", \"Makati\", \"Mandaluyong\")\n",
        "for item in seq:\n",
        "    # do stuff\n",
        "    pass\n",
        "\n",
        "# this is good, explicit\n",
        "cities = (\"Taguig\", \"Makati\", \"Mandaluyong\")\n",
        "for city in cities:\n",
        "    # do stuff\n",
        "    pass"
      ],
      "metadata": {
        "id": "FssfMEUmV0q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tip can save you if have a foreign collaborator that is not familiar with the city names in PH üòÖ"
      ],
      "metadata": {
        "id": "VbUx9yomMhwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a manual for your function using docstrings.** This will help not only you in the future, but also your future collaborators. üòâ\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "This is an example of Google style docstring.\n",
        "\n",
        "Args:\n",
        "    param1: This is the first param.\n",
        "    param2: This is a second param.\n",
        "\n",
        "Returns:\n",
        "    This is a description of what is returned.\n",
        "\n",
        "Raises:\n",
        "    KeyError: Raises an exception.\n",
        "\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "W-sDa_IQgOEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "# this is good, write docstrings as much as possible to future proof your work\n",
        "def get_haversine_distance(lon1, lat1, lon2, lat2, r=6371):\n",
        "    \"\"\"Calculate the great circle distance (in kilometers) between two points on the earth.\n",
        "\n",
        "    Args:\n",
        "        lon1 (float): Longitude of Point 1\n",
        "        lat1 (float): Latitude of Point 1\n",
        "        lon2 (float): Longitude of Point 2\n",
        "        lat2 (float): Latitude of Point 2\n",
        "        r (int, optional): Radius of earth in kilometers. Defaults to 6371.\n",
        "\n",
        "    Returns:\n",
        "        float: Haversine distance between the two given coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert decimal degrees to radians \n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # haversine formula \n",
        "    dlon = lon2 - lon1 \n",
        "    dlat = lat2 - lat1 \n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a)) \n",
        "    \n",
        "    return c * r"
      ],
      "metadata": {
        "id": "BJ9H4f9KgN8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your Python **functions should accomplish one thing.** When functions do more than one thing, they are harder to compose, test, and reason about. When you can isolate a function to just one action, they can be refactored easily and your code will read much cleaner. ü§ì"
      ],
      "metadata": {
        "id": "zin0BN-ATZBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "U9nI-j8_IHFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "id": "5aIwyCgzOIGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload();"
      ],
      "metadata": {
        "id": "0OiQ-cVpnoGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "1uW5DgIzIfzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(filename, schema):\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.astype(schema, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "filename = \"data/cafes_in_bgc.csv\"\n",
        "schema = {\n",
        "    \"cafe_name\": str,\n",
        "    \"address\": str,\n",
        "    \"latitude\": float,\n",
        "    \"longitude\": float\n",
        "} \n",
        "df = load_data(filename, schema)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "WJAQMt3UWw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you and your new DSP friends want to go coffee shop hopping in Bonifacio Global City today. Since you only have an hour for lunch break, you decided to only visit (n) shops for now. The task is to find the (n)-closest coffee shops to each other from the given data. üß© "
      ],
      "metadata": {
        "id": "ZFOnZlu3Wwsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import leafmap\n",
        "import itertools\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon"
      ],
      "metadata": {
        "id": "6L1PPJoxYLfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THE_GLOBE_TOWER_COORDS = (14.553474948859346, 121.04989287111896)"
      ],
      "metadata": {
        "id": "_-FhJtClftIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS BAD\n",
        "\n",
        "def get_map(df, reference_point, n=3):\n",
        "\n",
        "    # initialize map, set TGT as reference point for BGC\n",
        "    map_select = leafmap.Map(\n",
        "        center=reference_point, \n",
        "        zoom=16, \n",
        "        layers_control=True, \n",
        "        measure_control=False, \n",
        "        attribution_control=False\n",
        "    )\n",
        "    map_select.add_basemap(\"Stamen.TonerLite\")\n",
        "\n",
        "    # get points of interest df\n",
        "    gdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude, crs=\"EPSG:4326\"))\n",
        "    gdf_points = gdf_points.drop(columns=[\"address\", \"latitude\", \"longitude\"])\n",
        "\n",
        "    cols_gdf = [(f\"cafe_name_{i}\", f\"geometry_{i}\") for i in range(1, n+1)]\n",
        "    cols_gdf = [item for sublist in cols_gdf for item in sublist]\n",
        "    cols_geometry = [col for col in cols_gdf if \"geometry\" in col]\n",
        "\n",
        "    # get all possible combinations of poi(s) e.g. cafe(s)\n",
        "    points_combinations = list(itertools.combinations(gdf_points.values.tolist(), n))\n",
        "    \n",
        "    # get polygons df\n",
        "    gdf_polygons = pd.DataFrame(columns=cols_gdf)\n",
        "    for i, points_combination in enumerate(points_combinations):\n",
        "        gdf_polygons.loc[i] = [item for sublist in points_combination for item in sublist]\n",
        "\n",
        "    # add n-polygon geometry column based from the given points \n",
        "    gdf_polygons[\"geometry\"] = gdf_polygons.apply(lambda x: Polygon([x[col] for col in cols_geometry]), axis=1)\n",
        "    gdf_polygons = gdf_polygons.drop(columns=cols_geometry)\n",
        "\n",
        "    # 4326 for viz, 3857 for distance related calculations\n",
        "    gdf_polygons = gpd.GeoDataFrame(gdf_polygons, crs=\"EPSG:4326\")\n",
        "    gdf_polygons[\"polygon_perimeter_in_meters\"] = gdf_polygons.to_crs(3857)[\"geometry\"].length\n",
        "\n",
        "    # add the points and polygons gdf\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=True).head(1), layer_name=\"Smallest Geom\", fill_colors=[\"green\"])\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=False).head(1), layer_name=\"Biggest Geom\", fill_colors=[\"red\"])\n",
        "    map_select.add_gdf(gdf_points, layer_name=\"Cafes in BGC\")\n",
        "\n",
        "    return map_select\n"
      ],
      "metadata": {
        "id": "44ejlS75B8hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS BETTER\n",
        "\n",
        "def flatten_list(lst):\n",
        "    flattened_list = [item for sublist in lst for item in sublist]\n",
        "    return flattened_list\n",
        "\n",
        "def get_column_names(n, geom):\n",
        "    cols = flatten_list([(f\"cafe_name_{i}\", f\"geometry_{i}\") for i in range(1, n+1)])\n",
        "    if geom:\n",
        "        return [col for col in cols if \"geometry\" in col]\n",
        "    else:\n",
        "        return cols\n",
        "\n",
        "def get_point_combinations(gdf_points, n):\n",
        "    points_combinations = list(itertools.combinations(gdf_points.values.tolist(), n))\n",
        "    return points_combinations\n",
        "\n",
        "def get_gdf_points(df):\n",
        "    gdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude, crs=\"EPSG:4326\"))\n",
        "    gdf_points = gdf_points.drop(columns=[\"address\", \"latitude\", \"longitude\"])\n",
        "    return gdf_points\n",
        "\n",
        "def get_gdf_polygons(gdf_points, n):\n",
        "    \n",
        "    # get column names\n",
        "    cols_gdf = get_column_names(n, False)\n",
        "    cols_geometry = get_column_names(n, True)\n",
        "\n",
        "    # get all possible combinations of poi(s) e.g. cafe(s)\n",
        "    points_combinations = get_point_combinations(gdf_points, n)\n",
        "\n",
        "    # create polygons table\n",
        "    gdf_polygons = pd.DataFrame(columns=cols_gdf)\n",
        "    for i, points_combination in enumerate(points_combinations):\n",
        "        gdf_polygons.loc[i] = flatten_list(points_combination)\n",
        "\n",
        "    # add n-polygon geometry column based from the given points \n",
        "    gdf_polygons[\"geometry\"] = gdf_polygons.apply(lambda x: Polygon([x[col] for col in cols_geometry]), axis=1)\n",
        "    gdf_polygons = gdf_polygons.drop(columns=cols_geometry)\n",
        "    \n",
        "    # 4326 for viz, 3857 for distance related calculations\n",
        "    gdf_polygons = gpd.GeoDataFrame(gdf_polygons, crs=\"EPSG:4326\")\n",
        "    gdf_polygons[\"polygon_perimeter_in_meters\"] = gdf_polygons.to_crs(3857)[\"geometry\"].length\n",
        "\n",
        "    return gdf_polygons\n",
        "\n",
        "def get_map(reference_point, gdf_points, gdf_polygons):\n",
        "\n",
        "    # initialize map, set TGT as reference point for BGC\n",
        "    map_select = leafmap.Map(\n",
        "        center=reference_point, \n",
        "        zoom=16, \n",
        "        layers_control=True, \n",
        "        measure_control=False, \n",
        "        attribution_control=False\n",
        "    )\n",
        "    map_select.add_basemap(\"Stamen.TonerLite\")\n",
        "\n",
        "    # add the points and polygons gdf\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=True).head(1), layer_name=\"Smallest Geom\", fill_colors=[\"green\"])\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=False).head(1), layer_name=\"Biggest Geom\", fill_colors=[\"red\"])\n",
        "    map_select.add_gdf(gdf_points, layer_name=\"Cafes in BGC\")\n",
        "\n",
        "    return map_select"
      ],
      "metadata": {
        "id": "mb5jHARBYLdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_points = get_gdf_points(df)\n",
        "gdf_points.head(1)"
      ],
      "metadata": {
        "id": "M5yDjAR8-zz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_polygons = get_gdf_polygons(gdf_points, 4)\n",
        "gdf_polygons.head(1)"
      ],
      "metadata": {
        "id": "dc-s-1_Iim0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=True).head(1)"
      ],
      "metadata": {
        "id": "HcXCE2jyBrM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=False).head(1)"
      ],
      "metadata": {
        "id": "XxFZ_OTmBsLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_map(THE_GLOBE_TOWER_COORDS, gdf_points, gdf_polygons)"
      ],
      "metadata": {
        "id": "9MPdfTUUiyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Can you help me write a docstring for the get_map function below?"
      ],
      "metadata": {
        "id": "U97bzfxjNeDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_map(reference_point, gdf_points, gdf_polygons):\n",
        "    # YOUR CODE GOES HERE\n",
        "    # YOUR CODE GOES HERE\n",
        "    # YOUR CODE GOES HERE\n",
        "\n",
        "    # initialize map, set a reference point for the map\n",
        "    map_select = leafmap.Map(\n",
        "        center=reference_point, \n",
        "        zoom=16, \n",
        "        layers_control=True, \n",
        "        measure_control=False, \n",
        "        attribution_control=False\n",
        "    )\n",
        "    map_select.add_basemap(\"Stamen.TonerLite\")\n",
        "\n",
        "    # add the points and polygons gdf\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=True).head(1), layer_name=\"Smallest Geom\", fill_colors=[\"green\"])\n",
        "    map_select.add_gdf(gdf_polygons.sort_values(by=\"polygon_perimeter_in_meters\", ascending=False).head(1), layer_name=\"Biggest Geom\", fill_colors=[\"red\"])\n",
        "    map_select.add_gdf(gdf_points, layer_name=\"Points of Interest\")\n",
        "\n",
        "    return map_select"
      ],
      "metadata": {
        "id": "6Pbah46rNtvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHAT WE'VE COVERED**\n",
        "- How to write clean variables\n",
        "    - Make use of meaningful and pronounceable variable names, if possible.\n",
        "    - Make your code readable and searchable with the use of constants.\n",
        "    - Make use of explicit variable names, especially in lists.\n",
        "- How to write clean functions\n",
        "    - Write docstrings containing the input/output args and description.\n",
        "    - Break down your functions to accomplish one thing. Don't repeat yourself."
      ],
      "metadata": {
        "id": "ygv8yIf3Nq6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Tested Code\n",
        "Just because you've counted all the trees **doesn't mean you've seen the forest.** ü§î"
      ],
      "metadata": {
        "id": "v3xpkyxixmHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, you should write tests for your data science projects because it:\n",
        "- allows collaborators to **understand your code better**\n",
        "- confirms that the code is **working as expected**\n",
        "- helps in detecting **edge cases** or scenarios\n"
      ],
      "metadata": {
        "id": "sMJrBurikgeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have this function that identifies the sentiment of an English text. üßê"
      ],
      "metadata": {
        "id": "1lgSZEZMcrc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def extract_sentiment(text: str):\n",
        "    \"\"\"Extract text sentiments using textblob library\n",
        "    Args:\n",
        "        text (str): English text\n",
        "    Returns:\n",
        "        float: Polarity of the sentiment ranging from -1 to 1\n",
        "    \"\"\"\n",
        "\n",
        "    text = TextBlob(text)\n",
        "    sentiment = text.sentiment.polarity\n",
        "    \n",
        "    return sentiment"
      ],
      "metadata": {
        "id": "z5XGMyO1cpAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be using this library for the first time, we don't know how it reacts to different scenarios. We want to make sure that this tool or model is reliable, so **we will be testing it against multiple text inputs**, from the obvious scenarios to the rare ones or the edge cases."
      ],
      "metadata": {
        "id": "VTk--yB7cl2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_sentiment(\"The weather is beautiful today!\")"
      ],
      "metadata": {
        "id": "ir5Y-FiweHbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_sentiment(\"I had a bad meeting yesterday.\")"
      ],
      "metadata": {
        "id": "H7hHdcEUemh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Check out how TextBlob performs using your own sentiment for what you feel today."
      ],
      "metadata": {
        "id": "jnrxaZ74OWwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "DnFNDOrPOXmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to be able to do this kind of testing next time, but it is better to do it in a modular kind of way. So we will be using *pytest* - it is a **framework that makes it easy to write small, readable tests**, and can scale to support complex functional testing for applications and libraries."
      ],
      "metadata": {
        "id": "VJr0dI6mfCBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir src"
      ],
      "metadata": {
        "id": "ERe2nogTOmWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir tests"
      ],
      "metadata": {
        "id": "GPFjI1csOni_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "6Ro89KrWis26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file src/sentiment.py\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_sentiment(text: str):\n",
        "    \"\"\"Extract text sentiments using textblob library\n",
        "    Args:\n",
        "        text (str): English text\n",
        "    Returns:\n",
        "        float: Polarity of the sentiment ranging from -1 to 1\n",
        "    \"\"\"\n",
        "\n",
        "    text = TextBlob(text)\n",
        "    sentiment = text.sentiment.polarity\n",
        "    \n",
        "    return sentiment"
      ],
      "metadata": {
        "id": "zkWnkUx1r9Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file tests/test_sentiment.py\n",
        "\n",
        "import sys\n",
        "import os.path\n",
        "sys.path.append(\n",
        "    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
        ")\n",
        "from src.sentiment import extract_sentiment\n",
        "\n",
        "def test_extract_sentiment_positive():\n",
        "\n",
        "    text = \"I did well on the exam last week.\"\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "def test_extract_sentiment_negative():\n",
        "\n",
        "    text = \"This workshop is pretty basic and boring!\"\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment < 0\n",
        "\n",
        "def test_extract_sentiment_neutral():\n",
        "\n",
        "    text = \"...\"\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment == 0\n",
        "\n",
        "def test_extract_sentiment_filipino():\n",
        "\n",
        "    text = \"Nakakaengganyo pakinggan ang guro namin sa workshop\"\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0"
      ],
      "metadata": {
        "id": "mAV7BUsXr1YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be calling the *pytest* from the terminal. This will loop through our script and run the functions that have a prefix of **test**. ü§Ø"
      ],
      "metadata": {
        "id": "1DnTBzB7korp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -vv tests/test_sentiment.py"
      ],
      "metadata": {
        "id": "uEUhTcrhr6UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Update the tests/test_sentiment.py and run your own test."
      ],
      "metadata": {
        "id": "Iyr5tAGkO5Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file tests/test_sentiment.py\n",
        "\n",
        "import sys\n",
        "import os.path\n",
        "sys.path.append(\n",
        "    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
        ")\n",
        "from src.sentiment import extract_sentiment\n",
        "\n",
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "RRPgTaECPBYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -vv tests/test_sentiment.py"
      ],
      "metadata": {
        "id": "29-jtthxPUjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pytest output shown, we can see the scenarios where the function fails (e.g. the positive and filipino test inputs) and succeeds. From this exercise, **we are not only able to know whether our function works as expected but also know why it doesn‚Äôt work.** Based on result of the positive test input, we know that this sentiment identifier model from textblob isn't correct all the time. As the developer, we can now make an informed decision on what to do next. This shows the value of testing your work before using it in production. ü§©"
      ],
      "metadata": {
        "id": "YngkEcwzujbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also test multiple inputs using ```pytest.mark.parametrize```"
      ],
      "metadata": {
        "id": "xt55hHiys22h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file tests/test_sentiment.py\n",
        "\n",
        "import sys\n",
        "import os.path\n",
        "sys.path.append(\n",
        "    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
        ")\n",
        "import pytest\n",
        "from src.sentiment import extract_sentiment\n",
        "\n",
        "test_inputs_positive = [\n",
        "    \"I am blessed with a wonderful family.\",\n",
        "    \"I am thankful for my company.\",\n",
        "    \"I am grateful for my friends.\"\n",
        "]\n",
        "\n",
        "test_inputs_negative = [\n",
        "    \"I feel bad for leaving the party early last night.\",\n",
        "    \"I am still disappointed from my performance last week.\",\n",
        "    \"I am too sick to travel tomorrow.\"\n",
        "]\n",
        "\n",
        "@pytest.mark.parametrize(\"text\", test_inputs_positive)\n",
        "def test_extract_sentiment_positive(text):\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "@pytest.mark.parametrize(\"text\", test_inputs_negative)\n",
        "def test_extract_sentiment_negative(text):\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment < 0"
      ],
      "metadata": {
        "id": "jLsCtJHhsfBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -vv tests/test_sentiment.py"
      ],
      "metadata": {
        "id": "k4X8LXg9yYi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There comes a time where the test cases in your script will be lengthy and comprehensive. We can choose to run a specific test function one at a time using this syntax ```pytest file.py::function_name``` üòÆ"
      ],
      "metadata": {
        "id": "TWEy80e3xhQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -vv tests/test_sentiment.py::test_extract_sentiment_positive"
      ],
      "metadata": {
        "id": "TEMK0hAkxh-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Run the specific test function for test_extract_sentiment_negative"
      ],
      "metadata": {
        "id": "ByWE2AB5Pf4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "0G0mEm_APf-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also choose to use the same test input data to different functions using ```pytest.fixture```"
      ],
      "metadata": {
        "id": "d9yl75i4zDuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file tests/test_sentiment.py\n",
        "\n",
        "import sys\n",
        "import os.path\n",
        "sys.path.append(\n",
        "    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
        ")\n",
        "import pytest\n",
        "from src.sentiment import extract_sentiment\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_data():\n",
        "    return \"I had mixed feelings about the concert last night.\"\n",
        "\n",
        "def test_extract_sentiment_positive(sample_data):\n",
        "\n",
        "    sentiment = extract_sentiment(sample_data)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "def test_extract_sentiment_negative(sample_data):\n",
        "\n",
        "    sentiment = extract_sentiment(sample_data)\n",
        "\n",
        "    assert sentiment < 0\n",
        "\n",
        "def test_extract_sentiment_neutral(sample_data):\n",
        "\n",
        "    sentiment = extract_sentiment(sample_data)\n",
        "\n",
        "    assert sentiment == 0"
      ],
      "metadata": {
        "id": "czPvQBvFxiF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -vv tests/test_sentiment.py"
      ],
      "metadata": {
        "id": "WaZ3e3q9zro9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHAT WE'VE COVERED**\n",
        "- How to structure a basic test project\n",
        "- How to use pytest in automating tests\n",
        "    - How to run pytest and understand its results\n",
        "    - How to test multiple inputs using pytest.mark.parametrize\n",
        "    - How to pass common data to different functions using pytest.fixture"
      ],
      "metadata": {
        "id": "Y9K1XW0RMzFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Performant Code\n",
        "Efficiency is **doing better** what is already being done. ü§î"
      ],
      "metadata": {
        "id": "EXsx9DB00AYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the following sections, we will be discussing some tips and tricks on **how to better optimize your code in terms of speed and memory utilization using the pandas library.** As developers, it pays off for us to read the official documentation of the packages we frequently use, it enables us to leverage on its strengths and quirks which improves the efficiency of our default processes, and who knows, maybe we'll discover something we can improve on in our future projects that we can share with everyone in the community! üòÅ"
      ],
      "metadata": {
        "id": "CJ7kci2mSzKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "metadata": {
        "id": "bqo-ArDMtm7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key data structure in pandas is called a ```DataFrame```. It is a two-dimensional table with rows and columns, which is similar to the tables in relational databases and R's dataframe. **One important thing to know is that pandas is column-major**, which means *consecutive elements in a column are stored next to each other in memory.* Since modern computers process sequential data more efficiently than non sequential data, **if a table is column-major, accessing its columns will be much faster than accessing its rows.** ü§Ø"
      ],
      "metadata": {
        "id": "pf3a8AJiL398"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate this particular quirk of pandas, we will be using the **taxis dataset** that is readily available in the ```seaborn``` package."
      ],
      "metadata": {
        "id": "WtEcd5DMOseL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.get_dataset_names()"
      ],
      "metadata": {
        "id": "T7ItYNlCDrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis = sns.load_dataset(\"taxis\")\n",
        "df_taxis.head()"
      ],
      "metadata": {
        "id": "1FFeyJ9HJ83X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_taxis))"
      ],
      "metadata": {
        "id": "QjJV9PuK-Lod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A column in pandas ```DataFrame``` is called a ```Series```. Basically, a ```DataFrame``` is just a collection of ```Series``` stored to next to each other in memory."
      ],
      "metadata": {
        "id": "ISetWs1zP_MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the column `pickup`, 1k loops\n",
        "%timeit -n1000 df_taxis[\"pickup\"]"
      ],
      "metadata": {
        "id": "i_e2jsVfJ_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the first row, 1k loops\n",
        "%timeit -n1000 df_taxis.iloc[0]"
      ],
      "metadata": {
        "id": "0BkGaIaPKH81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the seaborn taxi dataset, **accessing a row takes about 30-50x longer than accessing a column.**"
      ],
      "metadata": {
        "id": "-XucPYGmKPbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Try to time the process of fetching another column in the df_taxis `DataFrame`."
      ],
      "metadata": {
        "id": "GEbNWgyVQV94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "TZa3U2QfQV3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at a simple pandas operation that we can execute in different ways. Say we want to add a column `travel_time` in our `df_taxis` dataframe before that pertains to the **total travel time of the passenger**, we can approach this problem in three different ways.\n",
        "- Iterate over the rows in the `DataFrame` using `.iterrows()`\n",
        "- Use a `lambda` function together with `.apply()` on the `DataFrame`\n",
        "- Use the relevant `Series` and directly perform the operation onto them"
      ],
      "metadata": {
        "id": "fTDGYrPAEynW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.info()"
      ],
      "metadata": {
        "id": "I_nXRZCsEz2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice from the schema shown above using ```.info()``` that the pickup and dropoff columns were identified as objects by default. We want to cast this to a datetime object in order for us to perform accurate datetime related calculations (e.g. subtraction for the elapsed time)."
      ],
      "metadata": {
        "id": "UApG_fKdEVwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cast the relevant fields to datetime\n",
        "df_taxis[\"pickup\"] = pd.to_datetime(df_taxis[\"pickup\"])\n",
        "df_taxis[\"dropoff\"] = pd.to_datetime(df_taxis[\"dropoff\"])"
      ],
      "metadata": {
        "id": "3ExzXTLMP1Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.info()"
      ],
      "metadata": {
        "id": "_BrI6oRIETV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option #1: this is bad\n",
        "def get_travel_time(df_taxis):\n",
        "    travel_time = []\n",
        "    for idx, row in df_taxis.iterrows():\n",
        "        travel_time.append(row.dropoff-row.pickup)\n",
        "    return pd.Series(travel_time)\n",
        "%timeit -n10 df_taxis[\"travel_time_a\"] = get_travel_time(df_taxis)"
      ],
      "metadata": {
        "id": "uwrtctjv0iDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option #2: this is good\n",
        "%timeit -n10 df_taxis[\"travel_time_b\"] = df_taxis.apply(lambda x: x[\"dropoff\"]-x[\"pickup\"], axis=1)"
      ],
      "metadata": {
        "id": "BH_dHLlz26Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option #3: this is better\n",
        "%timeit -n10 df_taxis[\"travel_time_c\"] = df_taxis[\"dropoff\"] - df_taxis[\"pickup\"]"
      ],
      "metadata": {
        "id": "AEfOMTaa1XqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Options #2 and #3 leverages on the quirk of pandas being column-major, hence making it a speedier alternative than Option #1. One reason why option #3 is faster than option #2 is because the ```.apply()``` operation makes use of python loops under the hood. üí°"
      ],
      "metadata": {
        "id": "03eJ9rhNIuBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# different approach, same result\n",
        "df_taxis[[col for col in df_taxis.columns if \"travel_time\" in col]].head()"
      ],
      "metadata": {
        "id": "5OO-OlHM2k18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, suppose we want to access the **tip from the first row** of the seaborn taxis dataset."
      ],
      "metadata": {
        "id": "TU3TDfPwJgOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# option #1: this is bad\n",
        "%timeit -n1000 df_taxis.iloc[0][\"tip\"]"
      ],
      "metadata": {
        "id": "xjSTksEtIp0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option #2: this is good\n",
        "%timeit -n1000 df_taxis.loc[0, \"tip\"]"
      ],
      "metadata": {
        "id": "ETSZ8UiKJ-Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option #2: this is better     \n",
        "%timeit -n1000 df_taxis[\"tip\"][0]"
      ],
      "metadata": {
        "id": "EZxWMzXvIC6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When performing multiple slice operations, **always do the column-based slicing first**, since it leverages on the column-major quirk of pandas."
      ],
      "metadata": {
        "id": "fVu7KZOmIC1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've used pandas before, most likely you've seen this `SettingCopyWarning` message when you try to assign values to a subset of the data. First, let's try to understand this warning message, then let's look for ways to address it amd avoid it from appearing in the future."
      ],
      "metadata": {
        "id": "F4uDrGxfBDT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"default\")"
      ],
      "metadata": {
        "id": "D5Nf3vyEC1fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.tail(1)"
      ],
      "metadata": {
        "id": "SuR9gEuOCSvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to alter the color of taxi for the last row of the pandas dataframe"
      ],
      "metadata": {
        "id": "rGQRp9Y4ChVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis[\"color\"][len(df_taxis)-1] = \"yellow\""
      ],
      "metadata": {
        "id": "Dwtke1AJBDL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.tail(1)"
      ],
      "metadata": {
        "id": "e3mP8XwhDmdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It worked but pandas threw the `SettingWithCopyWarning` mentioned above."
      ],
      "metadata": {
        "id": "I1CGCjv1Do0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want change the credit card payment category to mastercard."
      ],
      "metadata": {
        "id": "BBJAeesbDzGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis[df_taxis[\"payment\"]==\"credit card\"][\"payment\"] = \"mastercard\""
      ],
      "metadata": {
        "id": "Z3yU_Tn1EZLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.tail(1)"
      ],
      "metadata": {
        "id": "PQtei0f9ElBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It didn't work and pandas threw the `SettingWithCopyWarning` error mentioned above."
      ],
      "metadata": {
        "id": "5CVKYpopEm_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas behaves this way we're trying to make an assignment to a `Copy` instead of a `View`.\n",
        "\n",
        "- `Copy` is a copy of the actual `DataFrame`. This will be thrown away as soon as the operation is done.\n",
        "- `View` is the actual `DataFrame` you want to work with"
      ],
      "metadata": {
        "id": "UdOdHyoxExOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid this error, we can use the `.loc(row_indexer, col_indexer)` operation in pandas."
      ],
      "metadata": {
        "id": "3quc4ihHFnfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's revert the changes from yellow to green\n",
        "df_taxis.loc[len(df_taxis)-1, \"color\"] = \"green\""
      ],
      "metadata": {
        "id": "dtZ0Bw-mF0by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.tail(1)"
      ],
      "metadata": {
        "id": "X5SxpbQ2Gv0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's retry to update the credit card to mastercard\n",
        "df_taxis.loc[df_taxis[\"payment\"]==\"credit card\", \"payment\"] = \"mastercard\""
      ],
      "metadata": {
        "id": "sBNCAhO6GPpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.tail(1)"
      ],
      "metadata": {
        "id": "iTJtu0bJGByz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we were both able to update the contents of the dataframe without the `SettingWithCopyWarning` message!"
      ],
      "metadata": {
        "id": "g7QzFgAzFnYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úã What about you? Can you update the null entries in payment with `rewards`?"
      ],
      "metadata": {
        "id": "Ebjgas_sQ5_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_taxis.loc[df_taxis.payment.isna()].head()"
      ],
      "metadata": {
        "id": "EjszL3i6SRb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES HERE"
      ],
      "metadata": {
        "id": "3YnOO0myRfQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "affected_indices = [7, 445, 491, 545, 621]\n",
        "df_taxis.loc[affected_indices]"
      ],
      "metadata": {
        "id": "cb5AXXLqSbBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from our several demonstrations, there is a lot of ways for solving things in pandas. Next time, we can leverage our knowledge of pandas being column-major **in order to speed-up the computations in our data pipeline processes and exploratory data analysis.** üòÅ"
      ],
      "metadata": {
        "id": "H1XF45FcPYq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sometimes, we process near-to-larger-than-memory datasets in adhoc,** and when our only option available is pandas, here are some tips and tricks on how to process large datasets efficiently without running out of memory or exhausting your compute resources."
      ],
      "metadata": {
        "id": "fEr-HTjXLk4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "m-go3P6htflX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "id": "KYd1hH47tX5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload();"
      ],
      "metadata": {
        "id": "p8R5KektFp0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "4klQvM-FqnIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you're loading data into pandas, **load only the relevant columns** otherwise they're just occupying unnecessary space in the memory."
      ],
      "metadata": {
        "id": "G2rXZxywLkx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/shivamb/netflix-shows\n",
        "FILEPATH_NETFLIX_TITLES = \"data/netflix_titles.csv\""
      ],
      "metadata": {
        "id": "QtEzKWwGvgJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_netflix = pd.read_csv(FILEPATH_NETFLIX_TITLES)\n",
        "df_netflix.info(verbose=True, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "ySnNJ7vLu1DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_netflix.head()"
      ],
      "metadata": {
        "id": "2PPJsKQxu6UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_netflix.nunique()"
      ],
      "metadata": {
        "id": "kHU3H6u_wdkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we're only tasked to **visualize the number of netflix movies released across the years since its inception.** If this dataset is heavy, we can opt not to load the other unncessary columns by using the parameter ```usecols``` in pandas' ```read_csv``` method and select the relevant ones. "
      ],
      "metadata": {
        "id": "_1XopUrAqfEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"show_id\", \"type\", \"release_year\"]\n",
        "df_netflix = pd.read_csv(FILEPATH_NETFLIX_TITLES, usecols=cols)\n",
        "df_netflix.info(verbose=False, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "DsZzpyKZu7NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.5e6 / 1.1e6"
      ],
      "metadata": {
        "id": "gxnugdJAIrXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray! We've been able to shrink the memory usage down to **~7x times** just by mindfully selecting the columns we need for analysis."
      ],
      "metadata": {
        "id": "2MA_TrbpIrPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_titles_over_the_yrs = df_netflix.groupby([\"release_year\", \"type\"])[\"show_id\"].nunique().reset_index(drop=False)\n",
        "netflix_titles_over_the_yrs.head()"
      ],
      "metadata": {
        "id": "3Epx92uYzNVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 9))\n",
        "\n",
        "sns.lineplot(data=netflix_titles_over_the_yrs, x=\"release_year\", y=\"show_id\", hue=\"type\", ax=ax)\n",
        "plt.ylabel(\"# of titles\", fontsize=12)\n",
        "plt.xlabel(\"release year\", fontsize=12);"
      ],
      "metadata": {
        "id": "NjfZEIp4ywxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're able to accomplish the task without the other columns, right? Of couse, **err on the side of caution and do EDA and sanity checks first.** üòÖ"
      ],
      "metadata": {
        "id": "z7bI_T-Lw1e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "dQEeW5Qj7cla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "id": "MQJq6WYx2ZLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload();"
      ],
      "metadata": {
        "id": "otGfahBu2Z17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "zta-40z3NSUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case where you need to load many fields, but you still want to optimize on memory consumption, you can do the following:\n",
        "- Compress the categorical (usually a string) fields with the `category` dtype\n",
        "- Compress the numerical fields with `int` or `float` dtypes but with smaller magnitude"
      ],
      "metadata": {
        "id": "UHlWgOc62BkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/uciml/mushroom-classification\n",
        "FILEPATH_MUSHROOMS = \"data/mushrooms.csv\""
      ],
      "metadata": {
        "id": "kt4TkiXn9kL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mushrooms = pd.read_csv(FILEPATH_MUSHROOMS)\n",
        "df_mushrooms.info(verbose=True, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "ru_A5imy_Zn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, when loading the data into pandas, without passing a schema, **pandas just \"guesses\" the dtype of the fields.**"
      ],
      "metadata": {
        "id": "qHUtfQjZPDH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mushrooms.head()"
      ],
      "metadata": {
        "id": "ohFFCLEo_d83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mushrooms.nunique()"
      ],
      "metadata": {
        "id": "iBcr6Dv1NY7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon inspection, we see that the columns can fit in the `category` dtype, let's try passing a schema when loading the data."
      ],
      "metadata": {
        "id": "Mw-FhCBiP4fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mushrooms = pd.read_csv(FILEPATH_MUSHROOMS, dtype=\"category\")\n",
        "df_mushrooms.info(verbose=True, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "c4o-FfAvNQwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.3e6 / 193.3e3 "
      ],
      "metadata": {
        "id": "l3O3NIf1Rtrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray! We've been able to shrink the memory usage down to **~50x times** just by changing the original object/string dtype to `category`"
      ],
      "metadata": {
        "id": "IeytPcSxNXxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "metadata": {
        "id": "LaBJJ4aiSewl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "id": "yIVDr08yUJ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload();"
      ],
      "metadata": {
        "id": "lxZcNeOLUMh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "chNd5BKtRyHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
        "FILEPATH_MOBILE_PRICE = \"data/mobile_price.csv\""
      ],
      "metadata": {
        "id": "mCcB09H5Wf3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mobile_price = pd.read_csv(FILEPATH_MOBILE_PRICE)\n",
        "df_mobile_price.info(verbose=True, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "tk-MsiqTWfmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mobile_price.head()"
      ],
      "metadata": {
        "id": "t_GcNI9yWyvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mobile_price.nunique()"
      ],
      "metadata": {
        "id": "tezyKLqZXFQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mobile_price.describe().loc[[\"min\", \"max\"]]"
      ],
      "metadata": {
        "id": "cKz6ujN0W3QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just looking for floating points in the table\n",
        "\n",
        "def is_whole_number(n):\n",
        "    return n % 1 == 0\n",
        "\n",
        "df_mobile_price.apply(is_whole_number, axis=1).sum(axis=0) / len(df_mobile_price)"
      ],
      "metadata": {
        "id": "Ub2hEqG4YbHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's recall some basic computing concepts:\n",
        "\n",
        "For integers\n",
        "- **int8** can store integers from -128 to 127.\n",
        "- **int16** can store integers from -32768 to 32767.\n",
        "- **int64** can store integers from -9223372036854775808 to 9223372036854775807."
      ],
      "metadata": {
        "id": "7ff4IIhNXkwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"battery_power\": \"int16\",\n",
        "    \"blue\": bool,\n",
        "    \"clock_speed\": \"float32\",\n",
        "    \"dual_sim\": bool,\n",
        "    \"fc\": \"int8\",\n",
        "    \"four_g\": bool,\n",
        "    \"int_memory\": \"int8\",\n",
        "    \"m_dep\": \"float32\",\n",
        "    \"mobile_wt\": \"int16\",\n",
        "    \"n_cores\": \"int8\",\n",
        "    \"pc\": \"int8\",\n",
        "    \"px_height\": \"int16\",\n",
        "    \"px_width\": \"int16\",\n",
        "    \"ram\": \"int16\",\n",
        "    \"sc_h\": \"int8\",\n",
        "    \"sc_w\": \"int8\",\n",
        "    \"talk_time\": \"int8\",\n",
        "    \"three_g\": bool,\n",
        "    \"touch_screen\": bool,\n",
        "    \"wifi\": bool\n",
        "}\n",
        "df_mobile_price = pd.read_csv(FILEPATH_MOBILE_PRICE, dtype=schema)\n",
        "df_mobile_price.info(verbose=True, memory_usage=\"deep\")"
      ],
      "metadata": {
        "id": "DN8DDVOlXtWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "468.9e3 / 90.9e3 "
      ],
      "metadata": {
        "id": "wn2RfGgiXtQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray! We've been able to shrink the memory usage down to **~5x times** just by changing the original `int64` and `float64` dtypes identified by pandas to smaller magnitude integer and float dtype counterparts (and boolean) categories accordingly."
      ],
      "metadata": {
        "id": "SFELU18za9eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine this approach in an actual working dataset, you'll be surprised how efficient this technique can be in your own data science workflow, **not only it speeds up the subsequent processes, it also allows you to do more** since you have less usage of your compute resources. üòÅ"
      ],
      "metadata": {
        "id": "4HE0kyGSQItU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHAT WE'VE COVERED**\n",
        "- How to leverage pandas `DataFrame`'s quirk of being a column-major data structure.\n",
        "    - Use `Series` to speed up calculations as compared to a row-based approach.\n",
        "    - Efficient chaining of operation by starting with the column slicing.\n",
        "- How to correctly address the notorious `SettingWithCopy` warning of pandas.\n",
        "    - Difference between a `Copy` and `View`.\n",
        "    - How to mitigate the SettingWithCopy warning using `.loc()`.\n",
        "- How to efficiently load a pandas dataframe into memory.\n",
        "    - Use only relevant columns for the problem.\n",
        "    - Make use of the category dtype in pandas for categorical data.\n",
        "    - Make use of lower magnitude floats and integers for the data.\n"
      ],
      "metadata": {
        "id": "W8aDs7S43KNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What I expect from you after the session"
      ],
      "metadata": {
        "id": "DHErDZMTMnJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've finished this session I expect you to üìö‚úçÔ∏èüë®‚Äçüè´üë©‚Äçüè´üë®‚Äçüéìüë©‚Äçüéì"
      ],
      "metadata": {
        "id": "nUCLIgx8P_4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QUIZ\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSdPVkeKfi7ehcKpRCSQ8D8RMZAAVNqm8LeTZ8FH6NsobqHEwQ/viewform?usp=sf_link"
      ],
      "metadata": {
        "id": "ZLGNSrLKymV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REFERENCES"
      ],
      "metadata": {
        "id": "kAED596LT_gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some helpful references curated just for you!\n",
        "\n",
        "Docs\n",
        "- [Here is the PEP8 style guide in Python.](https://www.python.org/dev/peps/pep-0008/)\n",
        "- [Here is pendulum's official documentation.](https://pendulum.eustace.io/docs/)\n",
        "- [Here is pandas' official documentation.](https://pandas.pydata.org/docs/reference/index.html#api)\n",
        "- [Here is geopandas' official documentation.](https://geopandas.org/en/stable/docs.html)\n",
        "- [Here is pytest's official documentation.](https://docs.pytest.org/en/7.0.x/) \n",
        "- [Here is seaborn's official documentation.](https://seaborn.pydata.org/api.html)\n",
        "\n",
        "Books\n",
        "- [If you want to brush up on your python programming.](https://www.tomasbeuzen.com/python-programming-for-data-science/README.html)\n",
        "- [If you want to learn more about inferential thinking.](https://inferentialthinking.com/chapters/intro.html)\n",
        "- [If you want to learn more about geographic data science.](https://geographicdata.science/book/intro.html)\n",
        "\n",
        "Blogs\n",
        "- [If you want to learn more about test-driven development.](https://testdriven.io/blog/)"
      ],
      "metadata": {
        "id": "bfCfhNO5Ul6i"
      }
    }
  ]
}